{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5166a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import obonet\n",
    "import BOCC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import typing\n",
    "import seaborn as sns\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8052f750",
   "metadata": {},
   "source": [
    "## Plotting Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a91ab211",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_size = 16\n",
    "tick_label_size = 14\n",
    "panel_number_size = 24\n",
    "algos = ['paris-greedy','paris-walktrap','paris-infomap','paris-cesna']\n",
    "colors = ['#f4cccc','#fff2cc','#d9ead3','#a4c2f4']\n",
    "algos_colors = {x:c for x,c in zip(algos,colors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd5e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_ax(ax, top=False, bottom=False, left=False, right=False):\n",
    "    ax.spines['top'].set_visible(top)\n",
    "    ax.spines['bottom'].set_visible(bottom)\n",
    "    ax.spines['left'].set_visible(left)\n",
    "    ax.spines['right'].set_visible(right)\n",
    "    # ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(True)\n",
    "    ax.yaxis.set_tick_params(width=0.0, labelsize=8)\n",
    "    ax.xaxis.set_tick_params(width=0.0, labelsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10078d68",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7709056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the edgelists as networkx objects\n",
    "G15 = nx.read_edgelist('../Edgelists/String_HPO_2015.phenotypic_branch.edgelist.txt')\n",
    "G16 = nx.read_edgelist('../Edgelists/String_HPO_2016.phenotypic_branch.edgelist.txt')\n",
    "G17 = nx.read_edgelist('../Edgelists/String_HPO_2017.phenotypic_branch.edgelist.txt')\n",
    "G18 = nx.read_edgelist('../Edgelists/String_HPO_2018.phenotypic_branch.edgelist.txt')\n",
    "\n",
    "G19 = nx.read_edgelist('../Edgelists/String_HPO_2019.phenotypic_branch.edgelist.txt')\n",
    "G20 = nx.read_edgelist('../Edgelists/String_HPO_2020.phenotypic_branch.edgelist.txt')\n",
    "G21 = nx.read_edgelist('../Edgelists/String_HPO_2021.phenotypic_branch.edgelist.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87885a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load rediscovery stats\n",
    "g_snow = pd.read_csv('../SnowballResults/snowball.paris.greedy.String_HPO_2020.phenotypic_branch.tsv',sep='\\t')\n",
    "w_snow = pd.read_csv('../SnowballResults/snowball.paris.walktrap.String_HPO_2020.phenotypic_branch.tsv',sep='\\t')\n",
    "i_snow = pd.read_csv('../SnowballResults/snowball.paris.infomap.String_HPO_2020.phenotypic_branch.tsv',sep='\\t')\n",
    "c_snow = pd.read_csv('../SnowballResults/snowball.paris.cesna.String_HPO_2020.phenotypic_branch.tsv',sep='\\t')\n",
    "\n",
    "g_snow = pd.read_csv('../SnowballResultsFixed/snowball.paris.greedy.String_HPO_2020.phenotypic_branch.tsv',sep='\\t')\n",
    "w_snow = pd.read_csv('../SnowballResultsFixed/snowball.paris.walktrap.String_HPO_2020.phenotypic_branch.tsv',sep='\\t')\n",
    "i_snow = pd.read_csv('../SnowballResultsFixed/snowball.paris.infomap.String_HPO_2020.phenotypic_branch.tsv',sep='\\t')\n",
    "c_snow = pd.read_csv('../SnowballResultsFixed/snowball.paris.cesna.String_HPO_2020.phenotypic_branch.tsv',sep='\\t')\n",
    "print(g_snow.shape)\n",
    "print(w_snow.shape)\n",
    "print(i_snow.shape)\n",
    "print(c_snow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac94266f",
   "metadata": {},
   "source": [
    "## Plot network size stats by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dec79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count node and edge type stats\n",
    "def count_size_stats(_G):\n",
    "    # count genes\n",
    "    num_genes = sum(['HP:' not in n for n in _G.nodes])\n",
    "    # count HPOs\n",
    "    num_hpos = sum(['HP:' in n for n in _G.nodes])\n",
    "    # 0 = p2p, 1 = p2g, 2 = p2p\n",
    "    hpo_counts = [ sum(['HP:' in x for x in e]) for e in _G.edges]\n",
    "    # count g2p\n",
    "    num_g2p = hpo_counts.count(1)\n",
    "    # count g2g\n",
    "    num_p2p = hpo_counts.count(2)\n",
    "    # count p2p\n",
    "    num_g2g = hpo_counts.count(0)\n",
    "    return [num_genes, num_hpos, num_p2p, num_g2p, num_g2g]\n",
    "\n",
    "plt_data = {'year':[],'type':[],'count':[]}\n",
    "for y,_g in zip(list(range(2015,2022)),[G15,G16,G17,G18,G19,G20,G21]):\n",
    "    stats = count_size_stats(_g)\n",
    "    for t,x in zip(['STRING nodes', 'HPO Terms', 'HPO edges', 'gene-2-phenotyoe', 'STRING edges'],stats):\n",
    "        plt_data['year'].append(y)\n",
    "        plt_data['type'].append(t)\n",
    "        plt_data['count'].append(x)\n",
    "plt_df = pd.DataFrame(plt_data)\n",
    "\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42908000",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,15))\n",
    "g=sns.catplot(\n",
    "    data=plt_df, kind=\"bar\",\n",
    "    x=\"year\", y=\"count\", hue=\"type\",\n",
    "    ci=\"sd\")\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "g.set(yscale='log',xlabel='', facecolor='white')\n",
    "plt.savefig('../Figures/yearly_stats.jpg',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b6f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plt_df[plt_df['type'] == 'gene-2-phenotyoe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69fb011",
   "metadata": {},
   "source": [
    "## Calculate Log2Ratio and empirical p-value for each cluster in each algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a22060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_2_ratio(o: int, e: typing.List[int]) -> float:\n",
    "    \"\"\"\n",
    "    param o: observed number of rediscoveries\n",
    "    param e: list of expected number of rediscoveries based on the null model\n",
    "    \"\"\"\n",
    "    return float(np.log2(o / np.median(np.array(e))))\n",
    "\n",
    "def empircal_p(o: int, e: typing.List[int]) -> float:\n",
    "    \"\"\"\n",
    "    param o: observed number of rediscoveries\n",
    "    param e: list of expected number of rediscoveries based on the null model\n",
    "    \"\"\"\n",
    "    return float(1 - (sum([ o > x for x in e]) / len(e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee390e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2r_p = {'algorith':[],'cluster_id':[],'log2ratio':[],'p-value':[]}\n",
    "\n",
    "# test the functions\n",
    "assert(log_2_ratio(2,[2,2,2]) == 0)\n",
    "assert(log_2_ratio(2,[1,1,1]) > 0)\n",
    "assert(log_2_ratio(2,[3,3,3]) < 0)\n",
    "assert(empircal_p(2,[2,2,2]) == 1)\n",
    "assert(empircal_p(2,[1,1,1]) == 0)\n",
    "assert(round(empircal_p(2,[3,1,1]),2) == round(float(1/3),2))\n",
    "\n",
    "# calc log2ratio and p value for each cluster in each algo category. Save it all in one dataframe\n",
    "for df,algo in zip([g_snow, w_snow, i_snow, c_snow],algos):\n",
    "    for cid in df['com_id'].unique():\n",
    "        sub = df[df['com_id'] == cid]\n",
    "        observed = list(sub['com_score'])[0]\n",
    "        expected = list(sub['replicate_score'])\n",
    "        l2r = log_2_ratio(observed,expected)\n",
    "        p = empircal_p(observed,expected)\n",
    "        l2r_p['algorith'].append(algo)\n",
    "        l2r_p['cluster_id'].append(cid)\n",
    "        l2r_p['log2ratio'].append(l2r)\n",
    "        l2r_p['p-value'].append(p)\n",
    "l2r_p_df = pd.DataFrame(l2r_p)\n",
    "print(l2r_p_df.columns)\n",
    "print(l2r_p_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bbeb54",
   "metadata": {},
   "source": [
    "## Plot Log-2-Ratio vs P-Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eada53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "# for each algo\n",
    "l2r_p_df = pd.DataFrame(l2r_p)\n",
    "l2r_p_df = l2r_p_df.sort_values('log2ratio',ascending=False)\n",
    "arr = l2r_p_df['log2ratio']\n",
    "l2r_min = np.nanmin(arr[arr != -np.inf])\n",
    "l2r_max = np.nanmax(arr[arr != np.inf])\n",
    "print('Log-2-ratio non inf max and min', l2r_max, l2r_min)\n",
    "\n",
    "# replace INF with 12 (well above the next largest value) and -INF -7 (well below the next lowest)\n",
    "inf_replacement = 12\n",
    "neg_inf_replacement = -6\n",
    "# print(set(l2r_p_df['log2ratio']))\n",
    "# print(max([x for x in l2r_p_df['log2ratio'] if x != np.inf and not np.isnan(x)]))\n",
    "\n",
    "# l2r_p_df['log2ratio'] = [inf_replacement if x == np.inf else x for x in l2r_p_df['log2ratio']]\n",
    "# l2r_p_df['log2ratio'] = [neg_inf_replacement if x == -np.inf else x for x in l2r_p_df['log2ratio']]\n",
    "\n",
    "# a + 1\n",
    "# create figure and axes objects\n",
    "fig, axes = plt.subplots(12,2,gridspec_kw={'width_ratios': [3, 1],'height_ratios': [2, 4, 1, 2, 4, 1, 2, 4, 1, 2, 4, 1]})\n",
    "fig.set_size_inches(15, 15)\n",
    "\n",
    "# remove borders and ticks of plots that should be blank space\n",
    "for i in range(0,12):\n",
    "    for j in range(2):\n",
    "        clear_ax(axes[i][j])\n",
    "        if i % 3 == 0 and j == 1 or i in [2,5,8,11]:\n",
    "            axes[i][j].set_xticks([])\n",
    "            axes[i][j].set_yticks([])\n",
    "\n",
    "# labels for plot panels\n",
    "labels = ['A','B','C','D']\n",
    "for i,algo in enumerate(algos):\n",
    "    print(algo)\n",
    "    # establish proper indexes for histograms and scatter plots\n",
    "    hist_i = i * 3\n",
    "    scat_i = hist_i + 1\n",
    "    \n",
    "    # sub set the df for just this algo\n",
    "    sub = l2r_p_df[l2r_p_df['algorith'] == algo]\n",
    "    print('number of infs',sum(sub['log2ratio'] == inf_replacement), ',', sum(sub['log2ratio'] == inf_replacement) / sub.shape[0] )\n",
    "    print('number of -infs',sum(sub['log2ratio'] == neg_inf_replacement), ',', sum(sub['log2ratio'] == neg_inf_replacement) / sub.shape[0])\n",
    "    print('% r > 0', (sum(sub['log2ratio'] > 0) / sub.shape[0]) * 100, '%')\n",
    "    print('p < .0001', (sum(sub['p-value'] <= 0.0001) / sub.shape[0]) * 100, '%' )\n",
    "    # scatter\n",
    "    axes[scat_i][0].scatter(sub['log2ratio'],sub['p-value'],c = algos_colors[algo])\n",
    "    axes[scat_i][0].set_xlim(neg_inf_replacement,inf_replacement)\n",
    "    # set the x tick and artificially add\n",
    "    axes[scat_i][0].set_xticks([neg_inf_replacement,-4,-2,0,2,4,6,8,10,inf_replacement])\n",
    "    axes[scat_i][0].set_xticklabels(['-inf',-4,-2,0,2,4,6,8,10,'inf'])\n",
    "    axes[scat_i][0].set_ylim(-.2,1)\n",
    "    axes[scat_i][0].set_ylabel('{}\\np-value'.format(algo),size=label_size)\n",
    "    axes[scat_i][0].set_yticks([0,.5,1])\n",
    "    axes[scat_i][0].set_xlabel('log-2-ratio',size=label_size)\n",
    "    axes[scat_i][0].tick_params(axis='both', which='major', labelsize=tick_label_size)\n",
    "    \n",
    "    # X hist - above\n",
    "    hist_bins = axes[hist_i][0].hist(sub['log2ratio'],\n",
    "                                     bins=np.arange(neg_inf_replacement, inf_replacement + 1, 1),\n",
    "                                     orientation='vertical',\n",
    "                                     color = algos_colors[algo])\n",
    "    axes[hist_i][0].set_xlim(neg_inf_replacement,inf_replacement)\n",
    "    axes[hist_i][0].set_xticks([])\n",
    "    axes[hist_i][0].set_yscale('log')\n",
    "    axes[hist_i][0].set_ylabel('count',size=label_size)\n",
    "    axes[hist_i][0].tick_params(axis='both', which='major', labelsize=tick_label_size)\n",
    "    \n",
    "    # Panel label\n",
    "    axes[hist_i][0].text(-0.1, 1.15, labels[i], transform=axes[hist_i][0].transAxes,\n",
    "      fontsize=panel_number_size, fontweight='bold', va='top')\n",
    "    \n",
    "    # X hist - right\n",
    "    axes[scat_i][1].hist(sub['p-value'],bins=20,orientation='horizontal',color = algos_colors[algo])\n",
    "    axes[scat_i][1].set_ylim(-.2,1.1)\n",
    "    axes[scat_i][1].set_xscale('log')\n",
    "    axes[scat_i][1].set_yticks([])\n",
    "    axes[scat_i][1].set_xlabel('count',size=label_size)\n",
    "    axes[scat_i][1].tick_params(axis='both', which='major', labelsize=tick_label_size)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Figures/log_vs_p_19v20.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09527b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.log2())\n",
    "print(c_snow.columns)\n",
    "aa = c_snow[c_snow['com_id'] == 21364]\n",
    "print(aa)\n",
    "print(log_2_ratio(list(aa['com_score'])[0],list(aa['replicate_score'])))\n",
    "temp_o = list(aa['com_score'])[0]\n",
    "temp_e = list(aa['replicate_score'])\n",
    "print('O=',temp_o)\n",
    "print('E=',np.median(np.array(temp_e)))\n",
    "# float(np.log2(temp_o / np.median(np.array(temp_e))))\n",
    "print('Neg Inf')\n",
    "aa = c_snow[c_snow['com_id'] == 18210]\n",
    "print(aa)\n",
    "print(log_2_ratio(list(aa['com_score'])[0],list(aa['replicate_score'])))\n",
    "temp_o = list(aa['com_score'])[0]\n",
    "temp_e = list(aa['replicate_score'])\n",
    "print('O=',temp_o)\n",
    "print('E=',np.median(np.array(temp_e)))\n",
    "print(np.log2(temp_o / 10))\n",
    "l2r_p_df['p-value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771d0a5",
   "metadata": {},
   "source": [
    "## Edge Uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2p_edges_2021 = [e for e in G21.edges if sum('HP:' in x for x in e) == 1 and e not in G20.edges]\n",
    "print(len(g2p_edges_2021))\n",
    "# get a list of rediscoveries per algo\n",
    "def rediscover(edges,coms):\n",
    "    \"\"\"\n",
    "    @param edges: list of edges [[node1,node2],[node1,node3],..]\n",
    "    @param coms: list of BOCC objects\n",
    "    \"\"\"\n",
    "    rediscoveries = list()\n",
    "    for com in coms:\n",
    "        for edge in edges:\n",
    "            edge = list(edge)\n",
    "            if edge[0] in com.members and edge[1] in com.members:\n",
    "                edge.sort()\n",
    "                rediscoveries.append(str(edge))\n",
    "                print(com.name,edge)\n",
    "    return rediscoveries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict_2020 = {}\n",
    "for f in os.listdir('../SubComs/2020/'):\n",
    "    if 'paris' not in f:\n",
    "        continue\n",
    "    tmp_coms = BOCC.load_clusters('../SubComs/2020/' + f)\n",
    "    tmp_name = f.replace('.coms.txt','')\n",
    "    rediscover(g2p_edges_2021,tmp_coms)\n",
    "    clusters_dict_2020[tmp_name] = rediscover(g2p_edges_2021,tmp_coms)\n",
    "    print(tmp_name)\n",
    "    print(len(clusters_dict_2020[tmp_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc data for uniqueness\n",
    "uniq_rediscovery = {'algo':[],'infotype':[],'count':[]}\n",
    "# all_rediscoveries = list()\n",
    "# for key in rediscovery_res.keys():\n",
    "#     all_rediscoveries = all_rediscoveries + rediscovery_res[key]\n",
    "# all_rediscoveries_set = set(all_rediscoveries)\n",
    "\n",
    "for key in clusters_dict_2020.keys():\n",
    "    all_rediscoveries = list()\n",
    "    for key2 in clusters_dict_2020.keys():\n",
    "        if key == key2:\n",
    "            continue\n",
    "        all_rediscoveries = all_rediscoveries + clusters_dict_2020[key2]\n",
    "    \n",
    "    all_rediscoveries_set = set(all_rediscoveries)\n",
    "    print(len(all_rediscoveries_set))\n",
    "    print(key)\n",
    "    uniq_rediscovery['algo'].append(key)\n",
    "    uniq_rediscovery['infotype'].append('total')\n",
    "    uniq_rediscovery['count'].append(len(clusters_dict_2020[key]))\n",
    "    \n",
    "    uniq_rediscovery['algo'].append(key)\n",
    "    uniq_rediscovery['infotype'].append('total unique self')\n",
    "    uniq_rediscovery['count'].append(len(set(clusters_dict_2020[key])))\n",
    "    \n",
    "    uniq_rediscovery['algo'].append(key)\n",
    "    uniq_rediscovery['infotype'].append('common')\n",
    "    uniq_rediscovery['count'].append(len([x for x in set(clusters_dict_2020[key]) if x in all_rediscoveries_set]))\n",
    "    \n",
    "    uniq_rediscovery['algo'].append(key)\n",
    "    uniq_rediscovery['infotype'].append('unique')\n",
    "    uniq_rediscovery['count'].append(len([x for x in set(clusters_dict_2020[key]) if x not in all_rediscoveries_set]))\n",
    "\n",
    "uniq_rediscovery_df = pd.DataFrame(uniq_rediscovery)\n",
    "print(uniq_rediscovery_df.shape)\n",
    "assert(uniq_rediscovery_df.shape[0] == 4 * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a246f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "print(uniq_rediscovery_df)\n",
    "print('unique:',sum(uniq_rediscovery_df[uniq_rediscovery_df['infotype'] == 'unique']['count']))\n",
    "print('total unique self:',sum(uniq_rediscovery_df[uniq_rediscovery_df['infotype'] == 'total unique self']['count']))\n",
    "print('common:',sum(uniq_rediscovery_df[uniq_rediscovery_df['infotype'] == 'common']['count']))\n",
    "sns.barplot(data=uniq_rediscovery_df,x='algo',y='count',hue='infotype')\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "plt.savefig('../Figures/2021_rediscovery_on_2020_uniqness.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be929b27",
   "metadata": {},
   "source": [
    "## HPO to Gene Edges Infered from MPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300fa4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_from_mpo = pd.read_csv('../Resources/hpo_to_gene_derived_by_mpo.edgelist.2021.unique.txt',sep='\\t',header=None)\n",
    "print(hpo_from_mpo)\n",
    "hpo_from_mpo.columns = ['HPO','Gene','Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85970dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_mpo_edges = [[row['HPO'],row['Gene']] for i,row in hpo_from_mpo.iterrows()]\n",
    "hpo_mpo_clusters_dict_2021 = {}\n",
    "all_new_mpo_edges = []\n",
    "for f in os.listdir('../SubComs/2021/'):\n",
    "    if 'paris' not in f:\n",
    "        continue\n",
    "    tmp_coms = BOCC.load_clusters('../SubComs/2021/' + f)\n",
    "    tmp_name = f.replace('.coms.txt','')\n",
    "    hpo_mpo_clusters_dict_2021[tmp_name] = rediscover(hpo_mpo_edges,tmp_coms)\n",
    "    print(tmp_name)\n",
    "    print(len(hpo_mpo_clusters_dict_2021[tmp_name]))\n",
    "#     print(hpo_mpo_clusters_dict_2021[tmp_name])\n",
    "    all_new_mpo_edges = all_new_mpo_edges + hpo_mpo_clusters_dict_2021[tmp_name]\n",
    "\n",
    "print(set(all_new_mpo_edges))\n",
    "print(len(set(all_new_mpo_edges)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fcd8cc",
   "metadata": {},
   "source": [
    "## Mouse Snowballing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "mouse_snow_dfs = []\n",
    "mouse_files = ['snowball.paris.greedy.String_HPO_2021.mouse.phenotypic_branch.tsv',\n",
    "              'snowball.paris.walktrap.String_HPO_2021.mouse.phenotypic_branch.tsv',\n",
    "              'snowball.paris.infomap.String_HPO_2021.mouse.phenotypic_branch.tsv',\n",
    "              'snowball.paris.cesna.String_HPO_2021.mouse.phenotypic_branch.tsv']\n",
    "\n",
    "for i,file in enumerate(mouse_files):\n",
    "    tmp_df = pd.read_csv('../MouseSnowballResults/'+file,sep='\\t')\n",
    "    mouse_snow_dfs.append(tmp_df)\n",
    "\n",
    "\n",
    "mouse_l2r_p = {'algorithm':[],'cluster_id':[],'log2ratio':[],'p-value':[]}\n",
    "algos = ['paris-greedy','paris-walktrap','paris-infomap','paris-cesna']\n",
    "# print stats for all DFs\n",
    "# calc log2ratio and p value for each cluster in each algo category. Save it all in one dataframe\n",
    "for df,algo in zip(mouse_snow_dfs, algos):\n",
    "    for cid in df['com_id'].unique():\n",
    "        sub = df[df['com_id'] == cid]\n",
    "        observed = list(sub['com_score'])[0]\n",
    "        expected = list(sub['replicate_score'])\n",
    "        l2r = log_2_ratio(observed,expected)\n",
    "        p = empircal_p(observed,expected)\n",
    "        mouse_l2r_p['algorithm'].append(algo)\n",
    "        mouse_l2r_p['cluster_id'].append(cid)\n",
    "        mouse_l2r_p['log2ratio'].append(l2r)\n",
    "        mouse_l2r_p['p-value'].append(p)\n",
    "mouse_l2r_p_df = pd.DataFrame(mouse_l2r_p)\n",
    "\n",
    "fig, axes = plt.subplots(4)\n",
    "for i,algo in enumerate(algos):\n",
    "    sub = mouse_l2r_p_df[mouse_l2r_p_df['algorithm'] == algo]\n",
    "    axes[i].hist(sub['p-value'])\n",
    "    axes[i].set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(mouse_l2r_p_df.columns)\n",
    "print(mouse_l2r_p_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811a5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mouse_l2r_p_df['p-value'].min())\n",
    "print(sum(mouse_l2r_p_df['p-value'] == mouse_l2r_p_df['p-value'].min()))\n",
    "print(mouse_l2r_p_df['p-value'].max())\n",
    "print(sum(mouse_l2r_p_df['p-value'] == mouse_l2r_p_df['p-value'].max()))\n",
    "\n",
    "fig, axes = plt.subplots(4)\n",
    "for i, df in enumerate(mouse_snow_dfs):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    synth_wins = 0\n",
    "    reals_wins = 0\n",
    "    for com_id in df['com_id'].unique():\n",
    "        sub = df[df['com_id'] == com_id]\n",
    "        xs.append(list(sub['com_score'])[0])\n",
    "        ys.append(sub['replicate_score'].max())\n",
    "        if list(sub['com_score'])[0] > 0:\n",
    "            print(algos[i], com_id, list(sub['com_score'])[0], sub['replicate_score'].max())\n",
    "        if list(sub['com_score'])[0] > 0 and  list(sub['com_score'])[0] > sub['replicate_score'].max():\n",
    "            reals_wins += 1\n",
    "        if sub['replicate_score'].max() > list(sub['com_score'])[0]:\n",
    "            synth_wins += 1\n",
    "#             print('Synthetic win:', algos[i], com_id, list(sub['com_score'])[0], sub['replicate_score'].max())\n",
    "    print('Total synthetic wins',synth_wins )\n",
    "    print(df.shape[0])\n",
    "    print('Total synthetic wins p-value',1 - (synth_wins / df.shape[0])  )\n",
    "    \n",
    "            \n",
    "    axes[i].scatter(xs,ys)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "The past two results show me that the vast majority of clusters have\n",
    "p = 1.0 where both the real cluster and synthetic replicate have 0 rediscoveries.\n",
    "However there are still cases where synthetic beats the real clusters in a minority of cases.\n",
    "To invert the problem there is a p=0.99 that the null model is better than the actual clusters.\n",
    "So based on the mouse edges, there is no difference between our clusters and snowball samplings.\n",
    "But in the 6 cases our clusters actually rediscover something, they are all p < 0.0001.\n",
    "I think part of null results is due to limited information.\n",
    "There were only 85 edges we could test with.\n",
    "Several orders of magnitude less than the rediscover sims from previous years.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0dad4f",
   "metadata": {},
   "source": [
    "## Distance between nodes with and without pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb45a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the unpruned version of a graph\n",
    "G20_no_prune = nx.read_edgelist('../Edgelists/String_HPO_2020.all_hpo.edgelist.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c8eeada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NEK2', 'SGK494', 'SERPINF2'] 3\n",
      "['DOCK8', 'ACTB', 'HP:0000252', 'AARS1'] 4\n",
      "['CIBAR1', 'HP:0001162', 'MKKS', 'HP:0001249', 'IMPA1'] 5\n",
      "['MPIG6B', 'HP:0001873', 'CDC42', 'ADCY1', 'HP:0000399', 'RIPOR2'] 6\n",
      "['MPIG6B', 'HP:0001873', 'CDC42', 'ADCY1', 'HP:0000399', 'RIPOR2']\n"
     ]
    }
   ],
   "source": [
    "# get all genes connected to HP:0000007 - Autosomal dominant inheritance\n",
    "neighbors = G20_no_prune.neighbors('HP:0000007')\n",
    "gene_neighs = [x for x in neighbors if 'HP:' not in x]\n",
    "# everything in gene_neighs has a path of length 3\n",
    "# find 2 genes with a shortest path > 2 in the pruned on\n",
    "import random\n",
    "random.seed(0)\n",
    "longest_shortest_len = 0\n",
    "longest_shortest = []\n",
    "for i in range(10000):\n",
    "    nodes = random.sample(gene_neighs, 2)\n",
    "    if 'protein1' in nodes or 'protein2' in nodes or 'Info' in nodes or 'entrez-gene-id<tab>entrez-gene-symbol<tab>HPO-Term-Name<tab>HPO-Term-ID<tab>Frequency-Raw<tab>Frequency-HPO<tab>Additional' in nodes:\n",
    "        continue\n",
    "    path = nx.shortest_path(G20, nodes[0], nodes[1])\n",
    "    if len(path) > longest_shortest_len:\n",
    "        longest_shortest = path\n",
    "        longest_shortest_len = len(path)\n",
    "        print(str(path),len(path))\n",
    "print(longest_shortest)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
